{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dataset\n",
    "data = pd.read_csv('BreastCancer_Data.csv')\n",
    "data.isna().sum()\n",
    "data = data.dropna(axis=1)\n",
    "print(len(data[0]))\n",
    "print(data['diagnosis'].value_counts())\n",
    "sb.countplot(data['diagnosis'],label=\"count\")\n",
    "#sb.pairplot(data,hue=\"diagnosis\")\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "#sb.heatmap(corr,annot=True,fmt='%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into features and labels\n",
    "X = data.iloc[:,2:].values\n",
    "y = data.iloc[:,1].values\n",
    "\n",
    "#Encoding the categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "#Splitting the data into 70% training set and 30% testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function of many Machine Learning models\n",
    "def models(X_train,y_train):\n",
    "    \n",
    "    #Using Logistic Resgression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    log = LogisticRegression(random_state=0)\n",
    "    log.fit(X_train,y_train)\n",
    "    \n",
    "    #Using Decision Tree algorithm\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    tree =  DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "    tree.fit(X_train,y_train)\n",
    "    \n",
    "    #Using Random Forest Classification algorithm\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    forest = RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\n",
    "    forest.fit(X_train,y_train)\n",
    "    \n",
    "    #Using K-Nearest Neighbour algorithm\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=13,p=2)\n",
    "    knn.fit(X_train,y_train)\n",
    "    \n",
    "    #using Support Vector Machines algorithm\n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC(kernel='linear',random_state=0)\n",
    "    svm.fit(X_train,y_train)\n",
    "    \n",
    "    #Printing accuracy scores\n",
    "    print('[0]Logistic Regression training accuracy score : ',log.score(X_train,y_train))\n",
    "    print('[1]Decision Tree training accuracy score : ',tree.score(X_train,y_train))\n",
    "    print('[2]Random Forest training accuracy score : ',forest.score(X_train,y_train))\n",
    "    print('[3]KNN accuracy training score : ',log.score(X_train,y_train))\n",
    "    print('[4]SVM accuracy training score : ',log.score(X_train,y_train))\n",
    "    \n",
    "    return log,tree,forest,knn,svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]Logistic Regression training accuracy score :  0.9899497487437185\n",
      "[1]Decision Tree training accuracy score :  1.0\n",
      "[2]Random Forest training accuracy score :  0.9949748743718593\n",
      "[3]KNN accuracy training score :  0.9899497487437185\n",
      "[4]SVM accuracy training score :  0.9899497487437185\n",
      "Model  0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       108\n",
      "           1       0.98      0.95      0.97        63\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "97.6608187134503\n",
      "\n",
      "Model  1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       108\n",
      "           1       0.88      0.95      0.92        63\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.93      0.94      0.93       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n",
      "93.56725146198829\n",
      "\n",
      "Model  2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       108\n",
      "           1       0.98      0.92      0.95        63\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.97      0.96      0.96       171\n",
      "weighted avg       0.97      0.96      0.96       171\n",
      "\n",
      "96.49122807017544\n",
      "\n",
      "Model  3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       108\n",
      "           1       0.98      0.90      0.94        63\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.95      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n",
      "95.90643274853801\n",
      "\n",
      "Model  4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       108\n",
      "           1       0.92      0.97      0.95        63\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.95      0.96      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n",
      "95.90643274853801\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Calling the fuction models\n",
    "model = models(X_train,y_train)\n",
    "\n",
    "#Testing accuracy and other metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(len(model)):\n",
    "  print('Model ',i)\n",
    "  #Check precision, recall, f1-score\n",
    "  print( classification_report(y_test, model[i].predict(X_test)) )\n",
    "  #Another way to get the models accuracy on the test data\n",
    "  print( accuracy_score(y_test, model[i].predict(X_test))*100)\n",
    "  print()#Print a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
      "\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Since the testing accuracy of Logistic Regression is higher than that of any other trained models we use the Logistic Regression model for classification\n",
    "#Print Prediction of Random Forest Classifier model\n",
    "pred = model[0].predict(X_test)\n",
    "print(pred)\n",
    "\n",
    "print()\n",
    "\n",
    "#Print the actual values\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107   1]\n",
      " [  3  60]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix visualization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test,pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
